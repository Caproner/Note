# Word Embedding

Word Embedding（词嵌入）指代将词转化为向量的任务。该任务的目标是让两个词对应的向量的计算有意义，例如：

+ 向量内积表示两个词的相似度
+ 向量间加减有意义，例如女王=国王-男性+女性

这里介绍的是word2vec

> 直到BERT提出来之前，word2vec都是主流的Word Embedding方式

## Word2Vec

word2vec的核心思想是利用文本中词汇的上下文关系来训练词向量。其可以基于两种任务之一来实现：

+ Skip-gram：已知词汇，预测词汇的上下文
+ CBOW：已知上下文，预测所缺词汇

两者都是通过构造单一隐藏层的线性神经网络来完成这个任务。

以Skip-gram为例，其输入为某个词的one-hot，输出为每个词能成为其上下文的概率。

>故输入和输出都是V维向量（其中V为训练文本的词汇量）

而目标值就是训练文本中每个词成为当前词的上下文的概率。

> 可以对词的上下文关系进行计数。假设B词是A词的上下文次数是3，C词是A词的上下文次数是6，其他词是A词的上下文次数是7，那么B词是A词的上下文概率就是exp(3)/(exp(3)+exp(6)+exp(7))，也就是次数统计起来做一次softmax。

完成训练之后，对模型输入词，隐藏层的值就是其Embedding，而隐藏层的维数就是向量的维数。

> 由于输入词是one-hot，对应的神经网络只会有对应为1的位置被激活，故词向量本质上就是每个词对应的位置的参数。
>
> Skip-gram取的就是输入层到隐藏层的参数，CBOW取的是隐藏层到输出层的参数。

### 负采样

现实中，词库的词数量会非常庞大，而由于每个词的上下文词量十分有限，故目标值向量会非常稀疏。

故对每个词都去计算其对词库里所有词的上下文softmax概率的开销非常大。

对这种正例少负例多的情况一般使用负采样技术。

例如说词库有100000个词，而词汇A的上下文词种类仅有6个。那么将这6个当做是正例，再随机取6K个其他词作为负例（一般K取5），用这些例子训练即可。

于是这个问题就可以看做是二分类问题，不需要计算softmax，只需要取所取样的所有词（包括正负例）对应在输出层的位置进行训练就行，正例的目标值为1，负例的目标值为0，输出层不使用softmax而是使用sigmoid就行了。